# Diffusion Bunny Pipeline Configuration
# Main configuration file for the movie-to-diffusion pipeline

# Project settings
project:
  name: "diffusion-bunny"
  version: "0.1.0"
  data_dir: "./data"
  output_dir: "./data/output"
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR

# Pipeline configuration
pipeline:
  stages: ["siamese", "extraction", "filtering", "detection", "captioning"]
  force_rerun: false
  parallel_processing: false  # Set to true when stages support it
  resume_from_stage: null  # null, siamese, extraction, filtering, detection, captioning, finetuning, inference
  
# Input configuration
input:
  # Project start point - where the pipeline operates
  project_root: "./pipeline_data/sprite"
  
  # Directory structure within project root
  movie_dir: "movie"              # Contains source movie files
  frames_dir: "frames"            # Will be created for extracted frames
  character_references_dir: "characters"  # Character reference images (optional)
  
  # Supported formats
  supported_video_formats: [".mp4", ".avi", ".mov", ".mkv", ".wmv"]
  supported_image_formats: [".jpg", ".jpeg", ".png", ".bmp"]

# Stage 0: Siamese Network Training
siamese:
  enabled: true
  backbone: "mobilenetv2"  # mobilenetv2, efficientnet-b0
  embedding_dim: 128
  
  # Training configuration
  training:
    epochs: 50
    batch_size: 32
    learning_rate: 0.001
    weight_decay: 1e-4
    optimizer: "adam"  # adam, sgd
    
    # Learning rate scheduler
    scheduler: "step"  # step, plateau
    scheduler_step_size: 20
    scheduler_gamma: 0.5
    scheduler_patience: 10  # for plateau scheduler
    
    # Loss function
    loss_type: "contrastive"  # contrastive, triplet
    margin: 1.0
    
    # Data generation
    pairs_per_epoch: 1000
    positive_ratio: 0.5  # Ratio of positive pairs
    val_split: 0.2
    image_size: 224
    num_workers: 4
    
    # Data augmentation
    augmentation:
      rotation_range: 15
      brightness_range: 0.2
      contrast_range: 0.2
    
    # Training device
    device: "auto"  # auto, cpu, cuda
  
  # Inference configuration
  similarity_threshold: 0.8
  similarity_metric: "cosine"  # cosine, euclidean
  cache_embeddings: true

# Stage 1: Frame Extraction
extraction:
  method: "uniform"  # uniform | interval | keyframe
  target_frames: 4000  # Target number of frames to extract uniformly
  interval_seconds: 2.0  # Used when method is "interval"
  keyframe_threshold: 0.3  # Sensitivity for keyframe detection (0.1-1.0)
  output_format: "jpg"
  quality: 95  # JPEG quality (1-100)
  max_frames: null  # Maximum frames to extract (null for no limit)
  resize_width: null  # Resize frames to width (null to keep original)
  resize_height: null  # Resize frames to height (null to keep original)

# Stage 2: Quality Filtering + Deduplication
filtering:
  blur_threshold: 50.0  # Minimum Laplacian variance to keep frame (lower = accept more frames)
  batch_size: 10  # Number of frames to process in each batch
  keep_best_per_batch: false  # Keep all frames that meet quality thresholds
  min_batch_quality: 0.05  # Lower minimum quality threshold
  brightness_threshold: [10, 245]  # Wider brightness range [min, max] brightness levels (0-255)
  contrast_threshold: 0.05  # Lower contrast threshold (0.0-1.0)
  
  # Deduplication settings (Stage 2.5)
  deduplication:
    enabled: true  # Enable perceptual hash deduplication
    method: "perceptual_hash"  # perceptual_hash, temporal, clustering
    similarity_threshold: 0.95  # Remove frames >95% similar (0.0-1.0)
    hash_size: 8  # pHash precision (8 or 16)
    keep_best_quality: true  # Keep highest quality frame from similar group
    min_group_size: 2  # Minimum frames in group to trigger deduplication

# Stage 3: Character Detection
detection:
  # Face Detection Method Selection
  face_detection_method: "yolo_anime"  # haar | yolo_anime
  
  # Haar Cascade Face Detection Settings
  haar_scale_factor: 1.05  # How much the image size is reduced at each scale (more thorough)
  haar_min_neighbors: 3   # How many neighbors each candidate rectangle should have to retain it (less strict)
  haar_min_size: [20, 20] # Minimum possible face size [width, height] (smaller faces)
  
  # YOLOv8 Anime Face Detection Settings
  yolo_anime:
    model_repo: "arnabdhar/YOLOv8-Face-Detection"
    model_file: "model.pt"
    confidence_threshold: 0.35  # Reduced from 0.5 to catch more faces
    device: "auto"  # auto, cpu, cuda
    cache_model: true
  
  # Character Recognition
  feature_method: "siamese"  # orb | sift | surf | siamese (Siamese network for anime characters)
  similarity_threshold: 0.8  # Minimum similarity to match character (0.0-1.0)
  
  # Siamese network settings (when feature_method is "siamese")
  siamese:
    model_path: "pipeline_data/sprite/siamese/model_weights.pth"  # Path to trained Siamese model
    similarity_metric: "cosine"  # cosine | euclidean
    device: "auto"  # auto, cpu, cuda
    cache_embeddings: true  # Cache character embeddings for faster inference
  max_faces_per_frame: 15  # Maximum faces to detect per frame (increased from 10)
  min_face_size: 25  # Minimum face size in pixels (reduced from 30)
  face_padding: 0.2  # Padding around detected face (0.0-1.0)
  
  # Performance Settings
  batch_size: 10  # Number of frames to process in each batch
  character_embedding_cache: true  # Cache character embeddings
  
  # Output
  save_detected_faces: true  # Save detected faces for visual inspection
  save_detected_frames: true  # Save full frames with face bounding boxes
  save_character_matches_only: false  # Save all frames with detections, not just character matches
  
  # Character visualization colors (BGR format for OpenCV)
  character_colors:
    ellie: [0, 255, 0]      # Green
    elder: [0, 0, 255]      # Red
    victoria: [255, 0, 0]   # Blue
    default: [0, 255, 255]  # Yellow for unknown characters

# Stage 4: Caption Generation
captioning:
  provider: "bedrock"  # bedrock | openai | anthropic | local
  model: "claude-3-haiku"  # Model name for the selected provider
  max_tokens: 100
  temperature: 0.7
  batch_size: 5  # Number of images to caption in one API call
  prompt_template: |
    Describe this image in detail for training an AI art model. 
    Focus on: character appearance, clothing, pose, setting, lighting, and style.
    Be specific and descriptive. Character: {character_name}
  fallback_providers: ["openai", "local"]  # Fallback order if primary fails
  retry_attempts: 3
  retry_delay: 1.0  # Seconds between retries

# Stage 4.5: LLM Analysis (Remote Character Identification)
llm_analysis:
  enabled: true  # Enable/disable LLM analysis stage
  model: "anthropic/claude-3-haiku"  # OpenRouter model to use
  max_tokens: 500  # Increased for longer SD captions
  temperature: 0.7
  
  # Processing settings
  process_frames_with_faces_only: true  # Only analyze frames with detected faces
  min_face_confidence: 0.5  # Minimum face detection confidence to process
  batch_size: 5  # Number of frames to process in each batch
  
  # Debug settings
  debug_mode: false  # Set to true to limit frames for testing
  debug_max_frames: 3  # Maximum frames to process in debug mode (ignored when debug_mode is false)
  
  # Composite image settings
  strip_width: 150  # Width of character reference strip
  character_image_size: 150  # Size of each character image in strip
  strip_background_color: [240, 240, 240]  # Light gray background (BGR)
  
  # Cost optimization
  enable_caching: true  # Cache LLM responses to avoid duplicate API calls
  similarity_threshold_for_caching: 0.95  # Similarity threshold for cache hits
  max_requests_per_minute: 30  # Rate limiting for API requests
  
  # OpenRouter API settings
  openrouter:
    api_key_env: "OPENROUTER_API_KEY"  # Environment variable for API key
    base_url: "https://openrouter.ai/api/v1"
    timeout: 30  # Request timeout in seconds
    retry_attempts: 3  # Number of retry attempts for failed requests
    retry_delay: 1.0  # Base delay between retries (exponential backoff)

# Stage 5: Fine-tuning (LoRA/DreamBooth)
finetuning:
  # Model settings - CPU optimized
  #base_model: "Linaqruf/anything-v3.0" #anime
  base_model: "stable-diffusion-v1-5/stable-diffusion-v1-5"
  method: "lora"  # lora only
  output_dir: "outputs/lora_weights"
  device: "cpu"  # cpu | cuda
  
  # Data settings
  data_dir: "training_data/instance_images"  # Created by prepare_data.py
  instance_prompt: "anime character"
  resolution: 512
  center_crop: true
  random_flip: false
  
  # LoRA settings - CPU optimized (lower rank, UNet only)
  lora:
    rank: 8  # Lower for faster CPU training
    alpha: 16
    dropout: 0.1
    target_modules: ["to_k", "to_q", "to_v", "to_out.0"]
    train_text_encoder: false  # Only train UNet, freeze text encoder
  
  # Training settings - CPU optimized
  training:
    learning_rate: 5.0e-5  # Lower for stability
    batch_size: 1  # CPU limitation
    max_train_steps: 500  # Fewer steps for CPU
    gradient_accumulation_steps: 8  # Simulate larger batch
    gradient_checkpointing: true  # Save memory
    mixed_precision: "no"  # CPU doesn't support fp16
    seed: 42
    save_steps: 100  # Save checkpoint every 100 steps
    logging_steps: 10
    validation_prompt: "ellie, anime character, smiling"
    num_validation_images: 2
    
  # Prior preservation (optional, adds training time)
  prior_preservation:
    enabled: false  # Disable to save ~40% training time
    num_class_images: 200
    class_prompt: "anime character"
    class_data_dir: "regularization_images"
    prior_loss_weight: 1.0
  
  # Performance settings
  num_workers: 0  # CRITICAL for Windows
  pin_memory: false  # CPU doesn't benefit
  use_8bit_adam: false  # Not supported on CPU

# Stage 6: Inference
inference:
  model_path: "./data/models"  # Path to fine-tuned model
  adapter_path: "./data/models/lora"  # Path to LoRA adapters
  
  # Generation settings
  generation:
    num_images: 4
    num_inference_steps: 50
    guidance_scale: 7.5
    width: 512
    height: 512
    seed: null  # null for random seed
    
  # Output settings
  output:
    format: "png"
    quality: 95
    save_metadata: true

# External service configuration
aws:
  region: "us-east-1"
  profile: "default"  # AWS profile to use
  bedrock:
    model_id: "anthropic.claude-3-haiku-20240307-v1:0"
    max_tokens: 100
    temperature: 0.7

openai:
  model: "gpt-4-vision-preview"
  max_tokens: 100
  temperature: 0.7

anthropic:
  model: "claude-3-haiku-20240307"
  max_tokens: 100
  temperature: 0.7

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./logs/pipeline.log"
  max_file_size: "10MB"
  backup_count: 5

# Performance settings
performance:
  max_workers: 4  # Maximum number of worker threads
  chunk_size: 100  # Size of data chunks for processing
  memory_limit: "8GB"  # Maximum memory usage
  temp_dir: "./data/temp"  # Temporary files directory
  cleanup_temp: true  # Clean up temporary files after processing

# Development settings
development:
  debug_mode: false
  save_intermediate_results: true
  profile_performance: false
  verbose_logging: false
