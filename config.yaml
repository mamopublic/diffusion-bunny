# Diffusion Bunny Pipeline Configuration
# Main configuration file for the movie-to-diffusion pipeline

# Project settings
project:
  name: "diffusion-bunny"
  version: "0.1.0"
  data_dir: "./data"
  output_dir: "./data/output"
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR

# Pipeline configuration
pipeline:
  stages: ["extraction", "filtering", "detection", "captioning"]
  force_rerun: false
  parallel_processing: false  # Set to true when stages support it
  resume_from_stage: null  # null, extraction, filtering, detection, captioning, finetuning, inference
  
# Input configuration
input:
  video_path: "./data/input/movie.mp4"
  character_references_dir: "./data/input/characters"
  supported_video_formats: [".mp4", ".avi", ".mov", ".mkv", ".wmv"]
  supported_image_formats: [".jpg", ".jpeg", ".png", ".bmp"]

# Stage 1: Frame Extraction
extraction:
  method: "keyframe"  # keyframe | interval
  interval_seconds: 2.0  # Used when method is "interval"
  keyframe_threshold: 0.3  # Sensitivity for keyframe detection (0.1-1.0)
  output_format: "jpg"
  quality: 95  # JPEG quality (1-100)
  max_frames: 1000  # Maximum frames to extract (null for no limit)
  resize_width: null  # Resize frames to width (null to keep original)
  resize_height: null  # Resize frames to height (null to keep original)

# Stage 2: Quality Filtering
filtering:
  blur_threshold: 0.3  # Minimum blur score to keep frame (0.0-1.0)
  batch_size: 10  # Number of frames to process in each batch
  keep_best_per_batch: true  # Keep only the best frame per batch
  min_batch_quality: 0.1  # Minimum quality to keep any frame from batch
  brightness_threshold: [20, 235]  # [min, max] brightness levels (0-255)
  contrast_threshold: 0.1  # Minimum contrast score (0.0-1.0)

# Stage 3: Character Detection
detection:
  face_detection_model: "hog"  # hog | cnn | mediapipe
  similarity_threshold: 0.6  # Minimum similarity to match character (0.0-1.0)
  max_faces_per_frame: 5  # Maximum faces to detect per frame
  min_face_size: 50  # Minimum face size in pixels
  face_padding: 0.2  # Padding around detected face (0.0-1.0)
  enable_face_landmarks: false  # Extract facial landmarks
  character_embedding_cache: true  # Cache character embeddings

# Stage 4: Caption Generation
captioning:
  provider: "bedrock"  # bedrock | openai | anthropic | local
  model: "claude-3-haiku"  # Model name for the selected provider
  max_tokens: 100
  temperature: 0.7
  batch_size: 5  # Number of images to caption in one API call
  prompt_template: |
    Describe this image in detail for training an AI art model. 
    Focus on: character appearance, clothing, pose, setting, lighting, and style.
    Be specific and descriptive. Character: {character_name}
  fallback_providers: ["openai", "local"]  # Fallback order if primary fails
  retry_attempts: 3
  retry_delay: 1.0  # Seconds between retries

# Stage 5: Fine-tuning (LoRA/DreamBooth)
finetuning:
  base_model: "runwayml/stable-diffusion-v1-5"
  method: "lora"  # lora | dreambooth | both
  output_dir: "./data/models"
  
  # LoRA settings
  lora:
    rank: 16
    alpha: 32
    dropout: 0.1
    target_modules: ["to_k", "to_q", "to_v", "to_out.0"]
  
  # Training settings
  training:
    learning_rate: 1e-4
    batch_size: 1
    num_epochs: 10
    max_train_steps: 1000
    gradient_accumulation_steps: 4
    mixed_precision: "fp16"
    seed: 42
    
  # Data settings
  data:
    resolution: 512
    center_crop: true
    random_flip: true
    train_text_encoder: false

# Stage 6: Inference
inference:
  model_path: "./data/models"  # Path to fine-tuned model
  adapter_path: "./data/models/lora"  # Path to LoRA adapters
  
  # Generation settings
  generation:
    num_images: 4
    num_inference_steps: 50
    guidance_scale: 7.5
    width: 512
    height: 512
    seed: null  # null for random seed
    
  # Output settings
  output:
    format: "png"
    quality: 95
    save_metadata: true

# External service configuration
aws:
  region: "us-east-1"
  profile: "default"  # AWS profile to use
  bedrock:
    model_id: "anthropic.claude-3-haiku-20240307-v1:0"
    max_tokens: 100
    temperature: 0.7

openai:
  model: "gpt-4-vision-preview"
  max_tokens: 100
  temperature: 0.7

anthropic:
  model: "claude-3-haiku-20240307"
  max_tokens: 100
  temperature: 0.7

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./logs/pipeline.log"
  max_file_size: "10MB"
  backup_count: 5

# Performance settings
performance:
  max_workers: 4  # Maximum number of worker threads
  chunk_size: 100  # Size of data chunks for processing
  memory_limit: "8GB"  # Maximum memory usage
  temp_dir: "./data/temp"  # Temporary files directory
  cleanup_temp: true  # Clean up temporary files after processing

# Development settings
development:
  debug_mode: false
  save_intermediate_results: true
  profile_performance: false
  verbose_logging: false
