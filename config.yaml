# Diffusion Bunny Pipeline Configuration
# Main configuration file for the movie-to-diffusion pipeline

# Project settings
project:
  name: "diffusion-bunny"
  version: "0.1.0"
  data_dir: "./data"
  output_dir: "./data/output"
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR

# Pipeline configuration
pipeline:
  stages: ["extraction", "filtering", "detection", "captioning"]
  force_rerun: false
  parallel_processing: false  # Set to true when stages support it
  resume_from_stage: null  # null, extraction, filtering, detection, captioning, finetuning, inference
  
# Input configuration
input:
  # Project start point - where the pipeline operates
  project_root: "./pipeline_data/sprite"
  
  # Directory structure within project root
  movie_dir: "movie"              # Contains source movie files
  frames_dir: "frames"            # Will be created for extracted frames
  character_references_dir: "characters"  # Character reference images (optional)
  
  # Supported formats
  supported_video_formats: [".mp4", ".avi", ".mov", ".mkv", ".wmv"]
  supported_image_formats: [".jpg", ".jpeg", ".png", ".bmp"]

# Stage 1: Frame Extraction
extraction:
  method: "uniform"  # uniform | interval | keyframe
  target_frames: 4000  # Target number of frames to extract uniformly
  interval_seconds: 2.0  # Used when method is "interval"
  keyframe_threshold: 0.3  # Sensitivity for keyframe detection (0.1-1.0)
  output_format: "jpg"
  quality: 95  # JPEG quality (1-100)
  max_frames: null  # Maximum frames to extract (null for no limit)
  resize_width: null  # Resize frames to width (null to keep original)
  resize_height: null  # Resize frames to height (null to keep original)

# Stage 2: Quality Filtering
filtering:
  blur_threshold: 100.0  # Minimum Laplacian variance to keep frame (higher = sharper)
  batch_size: 10  # Number of frames to process in each batch
  keep_best_per_batch: true  # Keep only the best frame per batch
  min_batch_quality: 0.1  # Minimum quality to keep any frame from batch
  brightness_threshold: [20, 235]  # [min, max] brightness levels (0-255)
  contrast_threshold: 0.1  # Minimum contrast score (0.0-1.0)

# Stage 3: Character Detection
detection:
  # Face Detection Method Selection
  face_detection_method: "yolo_anime"  # haar | yolo_anime
  
  # Haar Cascade Face Detection Settings
  haar_scale_factor: 1.05  # How much the image size is reduced at each scale (more thorough)
  haar_min_neighbors: 3   # How many neighbors each candidate rectangle should have to retain it (less strict)
  haar_min_size: [20, 20] # Minimum possible face size [width, height] (smaller faces)
  
  # YOLOv8 Anime Face Detection Settings
  yolo_anime:
    model_repo: "Fuyucchi/yolov8_animeface"
    model_file: "yolov8x6_animeface.pt"
    confidence_threshold: 0.35  # Reduced from 0.5 to catch more faces
    device: "auto"  # auto, cpu, cuda
    cache_model: true
  
  # Character Recognition
  feature_method: "sift"  # orb | sift | surf (SIFT is more accurate for faces)
  similarity_threshold: 0.75  # Minimum similarity to match character (0.0-1.0) - increased for better precision
  max_faces_per_frame: 15  # Maximum faces to detect per frame (increased from 10)
  min_face_size: 25  # Minimum face size in pixels (reduced from 30)
  face_padding: 0.2  # Padding around detected face (0.0-1.0)
  
  # Performance Settings
  batch_size: 10  # Number of frames to process in each batch
  character_embedding_cache: true  # Cache character embeddings
  
  # Output
  save_detected_faces: true  # Save detected faces for visual inspection
  save_detected_frames: true  # Save full frames with face bounding boxes
  save_character_matches_only: true  # Only save frames with character matches
  
  # Character visualization colors (BGR format for OpenCV)
  character_colors:
    ellie: [0, 255, 0]      # Green
    elder: [0, 0, 255]      # Red
    victoria: [255, 0, 0]   # Blue
    default: [0, 255, 255]  # Yellow for unknown characters

# Stage 4: Caption Generation
captioning:
  provider: "bedrock"  # bedrock | openai | anthropic | local
  model: "claude-3-haiku"  # Model name for the selected provider
  max_tokens: 100
  temperature: 0.7
  batch_size: 5  # Number of images to caption in one API call
  prompt_template: |
    Describe this image in detail for training an AI art model. 
    Focus on: character appearance, clothing, pose, setting, lighting, and style.
    Be specific and descriptive. Character: {character_name}
  fallback_providers: ["openai", "local"]  # Fallback order if primary fails
  retry_attempts: 3
  retry_delay: 1.0  # Seconds between retries

# Stage 5: Fine-tuning (LoRA/DreamBooth)
finetuning:
  base_model: "runwayml/stable-diffusion-v1-5"
  method: "lora"  # lora | dreambooth | both
  output_dir: "./data/models"
  
  # LoRA settings
  lora:
    rank: 16
    alpha: 32
    dropout: 0.1
    target_modules: ["to_k", "to_q", "to_v", "to_out.0"]
  
  # Training settings
  training:
    learning_rate: 1e-4
    batch_size: 1
    num_epochs: 10
    max_train_steps: 1000
    gradient_accumulation_steps: 4
    mixed_precision: "fp16"
    seed: 42
    
  # Data settings
  data:
    resolution: 512
    center_crop: true
    random_flip: true
    train_text_encoder: false

# Stage 6: Inference
inference:
  model_path: "./data/models"  # Path to fine-tuned model
  adapter_path: "./data/models/lora"  # Path to LoRA adapters
  
  # Generation settings
  generation:
    num_images: 4
    num_inference_steps: 50
    guidance_scale: 7.5
    width: 512
    height: 512
    seed: null  # null for random seed
    
  # Output settings
  output:
    format: "png"
    quality: 95
    save_metadata: true

# External service configuration
aws:
  region: "us-east-1"
  profile: "default"  # AWS profile to use
  bedrock:
    model_id: "anthropic.claude-3-haiku-20240307-v1:0"
    max_tokens: 100
    temperature: 0.7

openai:
  model: "gpt-4-vision-preview"
  max_tokens: 100
  temperature: 0.7

anthropic:
  model: "claude-3-haiku-20240307"
  max_tokens: 100
  temperature: 0.7

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./logs/pipeline.log"
  max_file_size: "10MB"
  backup_count: 5

# Performance settings
performance:
  max_workers: 4  # Maximum number of worker threads
  chunk_size: 100  # Size of data chunks for processing
  memory_limit: "8GB"  # Maximum memory usage
  temp_dir: "./data/temp"  # Temporary files directory
  cleanup_temp: true  # Clean up temporary files after processing

# Development settings
development:
  debug_mode: false
  save_intermediate_results: true
  profile_performance: false
  verbose_logging: false
